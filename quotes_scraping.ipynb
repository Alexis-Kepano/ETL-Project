{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\Silho\\.wdm\\drivers\\chromedriver\\win32\\89.0.4389.23\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Quotes Page 10 Complete\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import time\n",
    "import pymongo\n",
    "\n",
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "browser.driver.maximize_window()\n",
    "# URL of page to be scraped\n",
    "url_base = \"http://quotes.toscrape.com/\"\n",
    "next_page_link=''\n",
    "\n",
    "quote_list=[]\n",
    "while True:\n",
    "    url=url_base+next_page_link\n",
    "    browser.visit(url)\n",
    "    html=browser.html\n",
    "    quote_soup=BeautifulSoup(html,'html.parser')\n",
    "    quotes=quote_soup.find_all('div',class_='quote')\n",
    "    #loops each quote on the web page quotes.toscrape.com\n",
    "    \n",
    "\n",
    "    for quote in quotes:\n",
    "        #finds the quote text\n",
    "        q_text = quote.find('span',class_=\"text\").text\n",
    "        #print(q_text)\n",
    "        #finds all tags \n",
    "        q_tags = quote.find_all('a',class_=\"tag\")\n",
    "\n",
    "        #list to store all tags\n",
    "        tag_list=[]\n",
    "        #loops through the quote tags and adds it to tag list\n",
    "        for tag in q_tags:\n",
    "            tag_text=tag.text\n",
    "            tag_list.append(tag_text)\n",
    "        \n",
    "        #visiting author page\n",
    "        #get author link\n",
    "        try:\n",
    "            #get link to author's page\n",
    "            author_link = quote.find('a')['href']\n",
    "            #print(author_link)\n",
    "            #got to author's page\n",
    "            browser.visit(url_base+author_link)\n",
    "#           time.sleep(10)\n",
    "            html2=browser.html\n",
    "            #get the html soup from author_link\n",
    "            author_soup=BeautifulSoup(html2,'html.parser')\n",
    "            #get author_details from author_soup\n",
    "            author_details=author_soup.find('div',class_='author-details')\n",
    "            #get author's name from author_details\n",
    "            author_name = author_details.find('h3').text\n",
    "            #get author's birth date and place from author_details \n",
    "            author_born=author_details.find('span',class_='author-born-date').text + \" \"+ author_details.find('span',class_='author-born-location').text\n",
    "            #get the description of the author\n",
    "            author_description=author_details.find('div',class_=\"author-description\").text\n",
    "            #creates a dictionary for quote, tag, author's name, born, and description \n",
    "            quote_dict={\"quote_text\":q_text,\"tags\":tag_list,\"author_name\":author_name,\"author_born\":author_born,\"author_description\":author_description}\n",
    "            #stores dictionary in a quote list\n",
    "            quote_list.append(quote_dict)\n",
    "            browser.back()\n",
    "        except Exception as e:\n",
    "            print(\"Scraping Author Page Complete\")\n",
    "            break\n",
    "            \n",
    "        \n",
    "            \n",
    "    try:\n",
    "        next_page = quote_soup.find('li',class_='next')\n",
    "        next_page_link = next_page.find('a')['href']\n",
    "        #browser.close()\n",
    "        #print(next_page_link)\n",
    "    except Exception as e:\n",
    "        print(f\"Scraping Quotes Page {x+1} Complete\")\n",
    "        break\n",
    "        #break\n",
    "\n",
    "browser.quit()  \n",
    "pprint(len(quote_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1b8e5c79b40>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "#creates a data base quotes_db\n",
    "db=client.quotes_db\n",
    "#creates a collection of quotes inside the quotes_db\n",
    "collection = db.quotes\n",
    "#insert quote_list as a MongoDB document\n",
    "collection.insert_many(quote_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
